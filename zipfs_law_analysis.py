# -*- coding: utf-8 -*-
"""Zipfs_law_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HZLQKDzFhMVpOoxeD6AdEZjAs9rD6YTV

# üìò Zipf's Law and Statistical Language Analysis

This notebook explores **Zipf's Law** using a real text corpus (e.g., *War and Peace* by Leo Tolstoy).  
It demonstrates how word frequency distribution in natural language follows a power-law relationship,  
where a few words occur very often while most appear rarely.

The analysis includes:
- Word frequency distribution and verification of Zipf's Law  
- Vocabulary coverage (how many words cover 90% of the text)  
- Word co-occurrence graph and identification of the "core language"  
- Visualization of the rank‚Äìfrequency product
"""

# ==================================================
# PROJECT: Statistical Language Processing (Zipf's Law)
# Author: ppaczek04
# ==================================================

import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
from collections import Counter

# --- 1. Load text file ---

# Huge thanks to the "project Gutenberg" which shares many famous books in txt format for free
# Make sure "war_and_peace.txt" is in the same folder
with open("war_and_peace.txt", "r", encoding="utf-8") as f:
    text = f.read()

print("Characters in text:", len(text))

"""## üîπ Text Preprocessing and Tokenization

The text is converted to lowercase, and only alphabetic characters are kept.  
We then split the text into individual tokens (words) using regular expressions.  
This step removes punctuation and ensures consistent word forms for frequency analysis.

"""

# --- 2. Basic cleaning and tokenization  ---
text = text.lower()
tokens = re.findall(r'\b[a-z√°√©√≠√≥√∂≈ë√∫√º≈±ƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º]+\b', text)

print("Number of tokens:", len(tokens))

# --- 3. Word frequency analysis ---
freq = Counter(tokens)
freq_df = pd.DataFrame(freq.items(), columns=["word", "frequency"])
freq_df = freq_df.sort_values(by="frequency", ascending=False).reset_index(drop=True)
freq_df["rank"] = freq_df.index + 1
freq_df["rank*freq"] = freq_df["rank"] * freq_df["frequency"]

freq_df.head(20)

"""## üìä Zipf's Law Visualization

According to Zipf's Law, the frequency of any word is inversely proportional to its rank.  
Here we plot the **log‚Äìlog relationship** between word rank and frequency.  
The blue points represent actual data, while the red line shows the *ideal Zipfian distribution*.


"""

# --- 4. Zipf‚Äôs Law Visualization ---
plt.figure(figsize=(8,6))

# Actual data (blue dots)
plt.loglog(freq_df["rank"], freq_df["frequency"], marker=".", linestyle='none', label="Actual Data", alpha=0.8)

# Ideal Zipf line (red)
# Build ideal frequencies: f = f1 / rank
f1 = freq_df["frequency"].iloc[0]  # frequency of the most common word
ranks = np.arange(1, len(freq_df) + 1)
ideal_freq = f1 / ranks
plt.loglog(ranks, ideal_freq, color="red", linewidth=2, label="Ideal Zipf‚Äôs Law")

plt.xlabel("Word Rank (log scale)")
plt.ylabel("Frequency (log scale)")
plt.title("Zipf's Law in the text")
plt.legend()
plt.show()

print(freq_df.head(10))
print("Average of rank √ó frequency:", round(freq_df["rank*freq"].mean(), 2))

"""## ‚öñÔ∏è Product of Rank √ó Frequency

Zipf's Law implies that the product `rank √ó frequency` should remain approximately constant.  
This plot helps visualize how closely our text follows this theoretical relationship.  
A stable horizontal line would indicate a perfect Zipfian pattern.

"""

# --- 4b. Product of Rank √ó Frequency (Zipf‚Äôs Law test) ---

plt.figure(figsize=(8,6))

# Extract rank and frequency values
ranks = freq_df["rank"].values
freqs = freq_df["frequency"].values
products = ranks * freqs

# Ideal constant (starting from rank 1000 to avoid noise)
ideal_product = products[999:].mean()

# Plot actual data for first 1000 and beyond
plt.plot(ranks[:1000], products[:1000], marker='.', label='Actual Data (first 1000)', alpha=0.7)
plt.plot(ranks[999:], products[999:], marker='.', label='Actual Data (from 1000th place)', alpha=0.7, color='orange')

# Ideal constant line
plt.axhline(y=ideal_product, color='r', linestyle='-', linewidth=2,
            label=f'Ideal Constant (from 1000th place) = {ideal_product:.2f}', alpha=0.8)

plt.title("Product: Rank √ó Frequency")
plt.xlabel("Word Rank")
plt.ylabel("Rank √ó Frequency")
plt.legend()
plt.grid(True)
plt.show()

"""## üìà Vocabulary Coverage (90%)

In this section, we calculate how many of the most frequent words account for **90% of all word occurrences** in the text.  
This measure shows how much vocabulary a reader must know to understand the majority of the text.  
The red dashed line marks the 90% threshold on the cumulative coverage curve.

"""

# --- 5. Vocabulary coverage for 90% of the text ---
total = sum(freq_df["frequency"])
freq_df["share"] = freq_df["frequency"] / total
freq_df["cumulative"] = freq_df["share"].cumsum()

coverage_90 = freq_df[freq_df["cumulative"] <= 0.9]
print(f"To cover 90% of the text, you need {len(coverage_90)} most frequent words.")

plt.figure(figsize=(8,4))

# Add horizontal dashed line at 0.9
plt.axhline(y=0.9, color='red', linestyle='--', linewidth=2, label='90% coverage')

plt.plot(freq_df["rank"], freq_df["cumulative"], color='blue', label="Cumulative share")
plt.xlabel("Rank")
plt.ylabel("Cumulative share")
plt.title("Text coverage by most frequent words")
plt.grid(True)
plt.show()

"""## üï∏Ô∏è Word Co-occurrence Network

In this section, we construct an **undirected co-occurrence graph**,  
where each node represents a unique word and each edge connects two words  
that appear next to each other in the text.  

This type of network helps visualize **syntactic and grammatical relationships** between words.  
Highly connected nodes (with many neighbors) usually correspond to:
- **Function words** such as *and, the, of, to, in*, which appear frequently across contexts, or  
- **Common connectors** that hold the sentence structure together.  

The resulting graph allows us to identify the **core language** ‚Äî  
a small set of words that form the backbone of most sentences in the corpus.  

By analyzing degree centrality (number of neighbors for each node),  
we can highlight which words play the most important structural roles in the text.  


"""

# --- 6. Co-occurrence graph ---
# Build undirected graph: words are connected if they appear next to each other
G = nx.Graph()
for i in range(len(tokens) - 1):
    w1, w2 = tokens[i], tokens[i+1]
    if G.has_edge(w1, w2):
        G[w1][w2]["weight"] += 1
    else:
        G.add_edge(w1, w2, weight=1)

# Degree centrality (number of neighbors)
degree_centrality = nx.degree_centrality(G)
deg_df = pd.DataFrame(list(degree_centrality.items()), columns=["word", "centrality"])
deg_df = deg_df.sort_values(by="centrality", ascending=False).reset_index(drop=True)

core_words = deg_df.head(50)
print("=== Core language (50 words with most neighbors) ===")
print(core_words)

"""And here is a final visualisation of the graph"""

# --- 6b. Visualize the co-occurrence graph ---

# Extract subgraph of the 50 most frequent words
top_words = set(freq_df.head(50)["word"])
subG = G.subgraph(top_words)

plt.figure(figsize=(10, 8))
pos = nx.spring_layout(subG, k=0.5, seed=42)

# Draw nodes and edges
nx.draw_networkx_nodes(subG, pos, node_color="skyblue", node_size=1200, alpha=0.8)
nx.draw_networkx_edges(subG, pos, width=1.0, alpha=0.5)
nx.draw_networkx_labels(subG, pos, font_size=10, font_weight="bold")

plt.title("Word Co-occurrence Network (Top 30 Words)")
plt.axis("off")
plt.show()

# --- 7. Most frequent nouns-like words (simple heuristic)
# We'll just take the 50 most frequent words longer than 3 characters (approx. content words)
noun_like = freq_df[freq_df["word"].str.len() > 3].head(50)
print("=== 50 most frequent content words ===")
print(noun_like)

# --- 8. Compare with Swadesh list ---
swadesh_list = [
    "i", "you", "he", "we", "this", "that", "who", "what", "where",
    "when", "how", "not", "all", "many", "one", "two", "big", "long",
    "small", "woman", "man", "person", "child", "fish", "bird", "dog",
    "tree", "leaf", "root", "bark", "flower", "grass", "fruit", "seed",
    "stone", "water", "fire", "earth", "wind", "sky", "sun", "moon",
    "star", "night", "day", "year", "hot", "cold", "full"
]

common = set(noun_like["word"]).intersection(swadesh_list)
print("=== Words shared with the Swadesh list ===")
print(common)